name: prod_skal_run_crawler

on:
  workflow_dispatch:
    inputs:
      url_input:
        description: 'Enter the URL'
        required: false
        default: ''
      max_urls:
        description: 'Maximum number of URLs'
        required: true
        default: 'All'
        type: choice
        options:
          - '10'
          - '100'
          - '1000'
          - 'All'
  push:
    branches:
      - main
      - issue_skal
      - test
  schedule:
    - cron: "40 17 * * 0" # 毎週日曜日 02時40分（JST）

env:
  RUBY_ENV: "production"
  CACHE_KEY: prod-main-20241101
  SOPS_AGE_KEY: ${{ secrets.SOPS_AGE_KEY }}

jobs:
  crawler_job:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v3
      - uses: actions/cache@v3
        with:
          path: /tmp/.buildx-cache
          key: ${{ runner.os }}-buildx-${{ env.CACHE_KEY }}-${{ github.sha }}
          restore-keys: |
            ${{ runner.os }}-buildx-${{ env.CACHE_KEY }}-
      - name: Build Decrypter
        run: cd decrypter && ./build.sh
      - name: Setup buildx
        run: docker buildx create --use
      - name: "[Invoked by push] Execute build docker"
        if: ${{ github.event_name == 'push' && always() }}
        env:
          DRY_RUN: "true"
        run: cd main && ./build.sh
      - name: "[Invoked by scheduler] Execute crawler"
        if: ${{ github.event_name == 'schedule' || github.event_name == 'workflow_dispatch' && always() }}
        env:
          DRY_RUN: "false"
          TARGET_URL: ${{ github.event.inputs.url_input }}
          MAX_URL: ${{ github.event.inputs.max_urls == 'All' && '1000000' || github.event.inputs.max_urls }}
        run: cd main && ./schedule_build.sh
      - run: |
          rm -rf /tmp/.buildx-cache
          mv /tmp/.buildx-cache-new /tmp/.buildx-cache
